{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WC_Classifier_V1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHEGMILpEoUO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5abbd2c9-33eb-4f7a-e6af-e95b3b215220"
      },
      "source": [
        "#Connect to GitHub data (STILL LOADS FLOWER DATA)\n",
        "\n",
        "!wget --no-check-certificate \\\n",
        "    https://github.com/GGEDPFJC/WaterColumn_Classifier_V1/master/train.zip?raw=true \\\n",
        "    -O /tmp/train.zip"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-03 02:43:26--  https://github.com/GGEDPFJC/WaterColumn_Classifier_V1/master/train.zip?raw=true\n",
            "Resolving github.com (github.com)... 140.82.121.3\n",
            "Connecting to github.com (github.com)|140.82.121.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 404 Not Found\n",
            "2021-11-03 02:43:26 ERROR 404: Not Found.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2APO0sgu5pg2"
      },
      "source": [
        "#Load training and validation data\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "local_zip = '/tmp/train.zip'\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "local_zip = '/tmp/valid.zip'\n",
        "\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\n",
        "zip_ref.extractall('/tmp')\n",
        "zip_ref.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ef_kEXF526p"
      },
      "source": [
        "# Directory with our training plume pictures\n",
        "train_plume_dir = os.path.join('/tmp/train/plume')\n",
        "\n",
        "# Directory with our training no plume pictures\n",
        "train_noplume_dir = os.path.join('/tmp/train/noplume')\n",
        "\n",
        "# Directory with our validation plume pictures\n",
        "valid_plume_dir = os.path.join('/tmp/valid/plume')\n",
        "\n",
        "# Directory with our validation no plume pictures\n",
        "valid_noplume_dir = os.path.join('/tmp/valid/noplume')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJToJ0Ha6Il-"
      },
      "source": [
        "#Check filenames\n",
        "\n",
        "train_plume_names = os.listdir(train_plume_dir)\n",
        "print(train_plume_names[:10])\n",
        "\n",
        "train_noplume_names = os.listdir(train_noplume_dir)\n",
        "print(train_grass_names[:10])\n",
        "\n",
        "validation_plume_names = os.listdir(valid_plume_dir)\n",
        "print(validation_plume_hames[:10])\n",
        "\n",
        "validation_noplume_names = os.listdir(valid_noplume_dir)\n",
        "print(validation_noplume_names[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Vqlk65X6kFy"
      },
      "source": [
        "#Check number of files\n",
        "\n",
        "print('total training plume images:', len(os.listdir(train_plume_dir)))\n",
        "print('total training no plume images:', len(os.listdir(train_noplume_dir)))\n",
        "print('total validation plume images:', len(os.listdir(valid_plume_dir)))\n",
        "print('total validation no plume images:', len(os.listdir(valid_noplume_dir)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vsnLkf66-s0"
      },
      "source": [
        "#Inspect images\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# graph parameters\n",
        "nrows = 4\n",
        "ncols = 4\n",
        "\n",
        "# iteration index\n",
        "pic_index = 0\n",
        "\n",
        "# Set up matplotlib fig, and size it to fit 4x4 pics\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(ncols * 4, nrows * 4)\n",
        "\n",
        "pic_index += 8\n",
        "next_plume_pic = [os.path.join(train_plume_dir, fname) \n",
        "                for fname in train_plume_names[pic_index-8:pic_index]]\n",
        "next_noplume_pic = [os.path.join(train_noplume_dir, fname) \n",
        "                for fname in train_noplume_names[pic_index-8:pic_index]]\n",
        "\n",
        "for i, img_path in enumerate(next_plume_pic + next_noplume_pic):\n",
        "  # Set up subplot; subplot indices start at 1\n",
        "  sp = plt.subplot(nrows, ncols, i + 1)\n",
        "  sp.axis('Off') # Don't show axes (or gridlines)\n",
        "\n",
        "  img = mpimg.imread(img_path)\n",
        "  plt.imshow(img)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ijoK09N074I3"
      },
      "source": [
        "#Preprocess Images\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# All images will be rescaled by 1./255\n",
        "train_datagen = ImageDataGenerator(rescale=1/255)\n",
        "validation_datagen = ImageDataGenerator(rescale=1/255)\n",
        "\n",
        "# Flow training images in batches of 120 using train_datagen generator\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        '/tmp/train/',  # This is the source directory for training images\n",
        "        classes = ['plume', 'no plume'],\n",
        "        target_size=(200, 200),  # All images will be resized to 200x200\n",
        "        batch_size=120,\n",
        "        # Use binary labels\n",
        "        class_mode='binary')\n",
        "\n",
        "# Flow validation images in batches of 19 using valid_datagen generator\n",
        "validation_generator = validation_datagen.flow_from_directory(\n",
        "        '/tmp/valid/',  # This is the source directory for training images\n",
        "        classes = ['plume', 'no plume'],\n",
        "        target_size=(200, 200),  # All images will be resized to 200x200\n",
        "        batch_size=19,\n",
        "        # Use binary labels\n",
        "        class_mode='binary',\n",
        "        shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vr3h-hO57_9c"
      },
      "source": [
        "#Define Model\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from itertools import cycle\n",
        "\n",
        "from sklearn import svm, datasets\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import label_binarize\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from scipy import interp\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(input_shape = (200,200,3)), \n",
        "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu), \n",
        "                                    tf.keras.layers.Dense(1, activation=tf.nn.sigmoid)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7gpMUle8Mcs"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twBm40GH8OT1"
      },
      "source": [
        "model.compile(optimizer = tf.optimizers.Adam(),\n",
        "              loss = 'binary_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvJFeqtm8W3V"
      },
      "source": [
        "#Train model (15 epochs in this line)\n",
        "\n",
        "history = model.fit(train_generator,\n",
        "      steps_per_epoch=8,  \n",
        "      epochs=15,\n",
        "      verbose=1,\n",
        "      validation_data = validation_generator,\n",
        "      validation_steps=8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ihH8RXqn8dT9"
      },
      "source": [
        "#Evaluate accuracy\n",
        "\n",
        "model.evaluate(validation_generator)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3apFLpk8qEM"
      },
      "source": [
        "#Use model for predictions\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from keras.preprocessing import image\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        " \n",
        "  # predicting images\n",
        "  path = '/content/' + fn\n",
        "  img = image.load_img(path, target_size=(200, 200))\n",
        "  x = image.img_to_array(img)\n",
        "  plt.imshow(x/255.)\n",
        "  x = np.expand_dims(x, axis=0)\n",
        "  images = np.vstack([x])\n",
        "  classes = model.predict(images, batch_size=10)\n",
        "  print(classes[0])\n",
        "  if classes[0]<0.5:\n",
        "    print(fn + \" contains a bubble plume\")\n",
        "  else:\n",
        "    print(fn + \" does not contain a bubble plume\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}